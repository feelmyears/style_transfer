{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA to Autoencoders\n",
    "##### EECS 495: Deep Learning from Scratch, Final Project\n",
    "##### By Philip Meyers IV (pmm432)\n",
    "\n",
    "For my final project, I wanted to look at the method of style transfer discussed in 'A Neural Algorithm of Artistic Style' by Gatys et al. The paper presents a unique and very successful approach to the problem of style transfer, or creating a composite image by combining the content of one image with the style of another. Although such task appears quite difficult, the process demonstrated by Gatys et al. is pretty simple to implement. (Running it, on the other hard, is far more intensive as I found out). Gatys et al. use activations from hidden layers in a CNN trained for image classification (specifically the VGG-Network) to define a new optimization problem. The researchers observed that the activations in the earlier layers offer representation of lower-level features and structures of images that we associate with the content of images. Similarly, the activations in the later layers are more closely tied to more global image properties, like an artist's painting style. Thus a composite image can be generated from a purely noisy image by learning values for each pixel such that the learned values maximize similarities to early layer activations of the content image and late layer activations of the style image. \n",
    "\n",
    "Below is my implementation of the style transfer net using VGG19 and Tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import scipy.io\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "\n",
    "class VGG19:\n",
    "    VGG19_URL = \"http://www.vlfeat.org/matconvnet/models/beta16/imagenet-vgg-verydeep-19.mat\"\n",
    "    VGG19_PATH = os.path.abspath(VGG19_URL.split('/')[-1])\n",
    "    \n",
    "    # From https://www.mathworks.com/help/nnet/ref/vgg19.html\n",
    "    # We only care about convolutional part and ignore everything that touches the fully-connected layers\n",
    "    VGG19_LAYERS = ( \n",
    "        'conv1_1', 'relu1_1', 'conv1_2', 'relu1_2', 'pool1', \n",
    "        'conv2_1', 'relu2_1', 'conv2_2', 'relu2_2', 'pool2', \n",
    "        'conv3_1', 'relu3_1', 'conv3_2', 'relu3_2', 'conv3_3', 'relu3_3', 'conv3_4', 'relu3_4', 'pool3',\n",
    "        'conv4_1', 'relu4_1', 'conv4_2', 'relu4_2', 'conv4_3', 'relu4_3', 'conv4_4', 'relu4_4', 'pool4',\n",
    "        'conv5_1', 'relu5_1', 'conv5_2', 'relu5_2', 'conv5_3', 'relu5_3', 'conv5_4', 'relu5_4',\n",
    "    )\n",
    "    \n",
    "    def ensure_exists(self):\n",
    "        if not os.path.isfile(self.VGG19_PATH):\n",
    "            print(\"No cache of VGG19 data found\")\n",
    "            print(\"Downloading from {} (this could take a while)...\".format(self.VGG19_URL))\n",
    "            urllib.request.urlretrieve(self.VGG19_URL, self.VGG19_PATH)\n",
    "            print(\"VGG19 data downloaded and stored at {}\".format(self.VGG19_PATH))\n",
    "        else:\n",
    "            print(\"Cached VGG19 data found at {}\".format(self.VGG19_PATH))\n",
    "    \n",
    "    def load_data(self, path):\n",
    "        self.ensure_exists()\n",
    "        \n",
    "        data = scipy.io.loadmat(path)\n",
    "        weights = data['layers'][0]\n",
    "        mean = data['normalization'][0][0][0]\n",
    "        mean_pixel = np.mean(mean, axis=(0, 1))\n",
    "        self.mean_pixel = mean_pixel\n",
    "        self.weights = weights\n",
    "    \n",
    "    def load_net(self, input_layer, pooling_type='max'):\n",
    "        if self.weights is None or self.mean_pixel is None:\n",
    "            raise Exception('Data not loaded!')\n",
    "        weights = self.weights\n",
    "        net = {}\n",
    "        prev = input_layer\n",
    "        for i, name in enumerate(self.VGG19_LAYERS):\n",
    "            if 'conv' in name:\n",
    "                kernels, bias = weights[i][0][0][0][0]\n",
    "                \n",
    "                # matlab is [width, height, channels_in, channels_out]\n",
    "                # tf is     [height, width, channels_in, channels_out]\n",
    "                kernels = np.transpose(kernels, (1,0,2,3))\n",
    "                bias = bias.reshape(-1)\n",
    "                            \n",
    "                conv = tf.nn.conv2d(prev, tf.constant(kernels), strides=(1,1,1,1), padding='SAME')\n",
    "                conv = tf.nn.bias_add(conv, bias)\n",
    "                prev = conv\n",
    "            elif 'relu' in name:\n",
    "                relu = tf.nn.relu(prev)\n",
    "                prev = relu\n",
    "            elif 'pool' in name:\n",
    "                pool = None\n",
    "                if pooling_type == 'avg':\n",
    "                    pool = tf.nn.avg_pool(prev, ksize=(1,2,2,1), strides=(1,2,2,1), padding='SAME')\n",
    "                else:\n",
    "                    pool = tf.nn.max_pool(prev, ksize=(1,2,2,1), strides=(1,2,2,1), padding='SAME')\n",
    "                prev = pool        \n",
    "            else:\n",
    "                raise Exception('Unrecognized layer `{}` encountered'.format(name))\n",
    "            \n",
    "            net[name] = prev\n",
    "        \n",
    "        return net\n",
    "\n",
    "def compute_features(image, vgg, feature_layers, pooling_type, gram, g):\n",
    "    shape = (1,) + image.shape\n",
    "    placeholder = tf.placeholder('float', shape=shape)\n",
    "    net = vgg.load_net(placeholder, pooling_type)\n",
    "    image_norm = image - vgg.mean_pixel\n",
    "\n",
    "    features = {}\n",
    "    with g.as_default(), tf.Session() as sess:\n",
    "        for l in feature_layers:\n",
    "            f = net[l].eval(feed_dict={placeholder:np.asarray([image_norm])})\n",
    "            if gram:\n",
    "                f = np.reshape(f, (-1, f.shape[3]))\n",
    "                f = np.matmul(f.T, f) / f.size\n",
    "            features[l] = f\n",
    "    return features\n",
    "\n",
    "def style_transfer(\n",
    "    vgg,\n",
    "    content_image, \n",
    "    style_image,\n",
    "    iterations,\n",
    "    content_weight,\n",
    "    style_weight,\n",
    "    pooling_type,\n",
    "    content_layers,\n",
    "    style_layers,\n",
    "    learning_rate,\n",
    "    beta1,\n",
    "    beta2,\n",
    "    epsilon,\n",
    "    checkpoint_iters):\n",
    "    \n",
    "    g = tf.Graph()\n",
    "    with g.as_default():\n",
    "        content_features = compute_features(content_image, vgg, content_layers, pooling_type, False, g)\n",
    "        style_features = compute_features(style_image, vgg, style_layers, pooling_type, True, g)\n",
    "\n",
    "        initial_image = tf.Variable(tf.random_normal((1,) + content_image.shape))\n",
    "        net = vgg.load_net(initial_image, pooling_type)\n",
    "        \n",
    "        content_loss = []\n",
    "        for l in content_layers:\n",
    "            content_loss.append(tf.nn.l2_loss(net[l]-content_features[l]) / content_features[l].size)\n",
    "        content_loss = reduce(tf.add, content_loss)\n",
    "        \n",
    "        style_loss = []\n",
    "        for l in style_layers:\n",
    "            layer = net[l]\n",
    "            _, height, width, number = map(lambda i: i.value, layer.get_shape())\n",
    "            size = height * width * number\n",
    "            feats = tf.reshape(layer, (-1, number))\n",
    "            gram = tf.matmul(tf.transpose(feats), feats) / size\n",
    "            style_gram = style_features[l]\n",
    "            style_loss.append(tf.nn.l2_loss(gram - style_gram) / style_gram.size)        \n",
    "        style_loss = reduce(tf.add, style_loss)\n",
    "        \n",
    "        total_loss = content_weight * content_loss + style_weight * style_loss\n",
    "        train = tf.train.AdamOptimizer(learning_rate, beta1, beta2, epsilon).minimize(total_loss)\n",
    "        \n",
    "        best_loss = float('inf')\n",
    "        best = None\n",
    "        did_improve = False\n",
    "        with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            for i in range(iterations):\n",
    "                did_improve = False\n",
    "                train.run()\n",
    "                loss = total_loss.eval()\n",
    "                if loss < best_loss:\n",
    "                    best_loss = loss\n",
    "                    best = initial_image.eval()\n",
    "                    did_improve = True\n",
    "                    \n",
    "                img_out = best.reshape(content_image.shape) + vgg.mean_pixel\n",
    "                yield (i, img_out, did_improve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.autolayout'] = True\n",
    "%matplotlib inline\n",
    "from matplotlib.pyplot import imshow\n",
    "import scipy.misc\n",
    "import time\n",
    "\n",
    "def read_img(path):\n",
    "    return scipy.misc.imread(path).astype(np.float)\n",
    "\n",
    "def save_img(path, img):\n",
    "    img = np.clip(img, 0, 255).astype(np.uint8)\n",
    "    Image.fromarray(img).save(path, quality=95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I explored this approach to style transfer on three pairs of images:\n",
    "\n",
    "* Content image of Main Library, style image of cubism\n",
    "![alt text](main.jpg \"Main Library\")\n",
    "![alt text](cubism.jpg \"Main Library\")\n",
    "* Content image of me, style image of Van Gogh's self portait\n",
    "![alt text](me.jpg \"Me\")\n",
    "![alt text](vg.jpg \"Van Gogh\")\n",
    "* Content image of me, style image of Naruto\n",
    "![alt text](me.jpg \"Me\")\n",
    "![alt text](naruto.jpg \"Me\")\n",
    "\n",
    "I explored the hyperparameter space of content weight, style weight, pooling type, and learning rate, generating as many possible combinations of the parameters as time permitted. \n",
    "\n",
    "Time was unexpectedly the biggest challenge of this project. I knew the optimization would be long, but I didn't know that it would be 2-hours-per-image long, even after I resized the image! When I attempted to reduce the size of the image to a point where the process only took 5-10 minutes, the images were way too small to be able to discern much detail or style. A big learning experience of this project was simply learning how to queue up a lot of work (**and** constantly save results--I lost about a day's worth of work because my computer crashed before I could save the results of multiple style transfers) and leave my computer to itself for a day or two to grind through the style transfers. \n",
    "\n",
    "The following cell was used to experiment with 2 pooling types (max and average), 3 content weights ($1/2, 5, 50), and 3 style weights (50, 500, 5000) for a total of hyperparameter combinations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_path = 'me.jpg'\n",
    "style_path = 'naruto.jpg'\n",
    "\n",
    "content_image = read_img(content_path)\n",
    "content_image = scipy.misc.imresize(content_image, 0.25)\n",
    "style_image = read_img(style_path)\n",
    "style_image = scipy.misc.imresize(style_image, 0.5)\n",
    "\n",
    "vgg = VGG19()\n",
    "vgg.load_data(vgg.VGG19_PATH)\n",
    "\n",
    "content_layers = ('relu4_2', 'relu5_2')\n",
    "style_layers = ('relu1_1', 'relu2_1', 'relu3_1', 'relu4_1', 'relu5_1')\n",
    "\n",
    "pooling_type ='max'\n",
    "epsilon = 1e-08\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "learning_rate = 1e1\n",
    "iterations = 1000\n",
    "content_weight = 5e0\n",
    "style_weight=5e2\n",
    "ct = 0\n",
    "best_images = []\n",
    "for pooling_type in ('max', 'avg'):\n",
    "    for cw in (-1, 0, 1):\n",
    "        content_weight = 5*10**cw\n",
    "        for sw in (1,2,3):\n",
    "            ct += 1\n",
    "            style_weight = 5*10**sw\n",
    "            \n",
    "            print(ct, pooling_type, content_weight, style_weight)\n",
    "\n",
    "            best_img = content_image\n",
    "            for i, img, did_improve in style_transfer(\n",
    "                vgg,\n",
    "                content_image, \n",
    "                style_image,\n",
    "                iterations,\n",
    "                content_weight,\n",
    "                style_weight,\n",
    "                pooling_type,\n",
    "                content_layers,\n",
    "                style_layers,\n",
    "                learning_rate,\n",
    "                beta1,\n",
    "                beta2,\n",
    "                epsilon,\n",
    "                checkpoint_iters=10):\n",
    "\n",
    "                if did_improve:\n",
    "                    best_img = img\n",
    "                    \n",
    "            hp = {\n",
    "                'ct': ct,\n",
    "                'content_weight': content_weight,\n",
    "                'cw':cw,\n",
    "                'style_weight':style_weight,\n",
    "                'sw': sw,\n",
    "                'pooling_type':pooling_type\n",
    "            }\n",
    "            \n",
    "            best_images.append((best_img, hp))\n",
    "\n",
    "for (img, hp) in best_images:\n",
    "    filename = 'me_naruto_cw{}_sw{}_{}.jpg'.format(hp['content_weight'], hp['style_weight'], hp['pooling_type'])\n",
    "    path = 'images/{}'.format(filename)\n",
    "    save_img(path, img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used the exact same setup to explore the same hyperparameter space for the other two image pairs. Furthermore, I was disappointed with the results from the initial Me/Naruto style transfer so I was just barely able to re-run all 18 combinations with a higher learning rate of 100 (instead of 10).\n",
    "\n",
    "The following is my favorite result from each of the image pairs:\n",
    "![alt text](images/me_naruto_cw50_sw5000_avg.jpg \"9th Hokage\")\n",
    "![alt text](images/me_vg_cw5_sw500_max.jpg \"Van Phil\")\n",
    "![alt text](images/main_cubism_cw50_sw5000_avg.jpg \"Main Cubism\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, an analysis of the results. The \"best looking\" images all had a content weight:style weight ratio of 1:100. When the ratio was lower than 1:100 (ie 1:10), the generated image maintained too many of the details from the content image:\n",
    "![](images/main_cubism_cw5_sw50_avg.jpg)\n",
    "And when the ratio was much higher than 1:100, almost all of the original content was lost and the generated image looked like a distorted version of the style image:\n",
    "![](images/main_cubism_cw0.5_sw5000_avg.jpg)\n",
    "When only the pooling type was varied, max pooling always yielded a more \"transformed\" image. Sometimes this distortion came as a better or stronger presence of style:\n",
    "![](images/me_vg_cw50_sw500_avg.jpg)\n",
    "![](images/me_vg_cw50_sw500_max.jpg)\n",
    "But other times the average pooling yielded an acceptable result while the max pooling yielded an incomprehsible result:\n",
    "![](images/me_naruto_cw5_sw5000_avg.jpg)\n",
    "![](images/me_naruto_cw5_sw5000_max.jpg)\n",
    "To my disappointment, running the Me/Naruto image pair did not yield better results with the higher learning rate. Perhaps the images are too dissimilar or perhaps the Naruto image simply doesn't contain enough \"style\" information in the chosen VGG-19 layers. Given more time (and a more reliable computer), I would love to explore the impact of choice of style and content layers on style transfer with various forms of content and style (rather than relying on the observations and conclusions offered by Gatys et al. and the Internet). The approach I would like to take is choose style and content layers based on their ability to maximally communicate information about a given content or style. For style, I envision doing this by collecting many images and grouping them by category (i.e impressionist paintings, cubism, water paints, etc.), feed those images through VGG-19 and collect their activations at each hidden layer, then evaluate each layer by category by how well it clusters the category's images in its output space. My reasoning is that if a certain layer maps a certain category of styles (like impressionist paintings) closer together than other layers, then perhaps that layer has a better embedding for that specific style. The same could be done for content images. \n",
    "\n",
    "This project has offered a lot of insight into the potential and difficulty of optimization with neural nets. It is incredible how well these newer methods can work, and can still be pretty fascinating when they don't work as planned:\n",
    "![](images/me_vg_cw0.5_sw5000_max.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
